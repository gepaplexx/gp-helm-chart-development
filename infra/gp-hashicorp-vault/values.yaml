backup:
  enabled: true
  schedule: '0 0 * * *'
  size: 2Gi
  retain:
    days: 31 # need to +1, so 30 days = 31
ingress:
  annotations:
    nginx.ingress.kubernetes.io/backend-protocol: HTTPS
    route.openshift.io/termination: edge
    letsencrypt.org/issuer: letsencrypt-production
  hostname: sso.apps.cluster.example.com



vault:
  global:
    openshift: true
  ui:
    enabled: true
  injector:
    enabled: true
    resources:
      requests:
        memory: '100Mi'
        cpu: '100m'
      limits:
        memory: '150Mi'
  server:
    dataStorage:
      size: '500Mi'
    standalone:
      enabled: false
    updateStrategyType: RollingUpdate
    readinessProbe:
      path: "/v1/sys/health?standbyok=true&sealedcode=204&uninitcode=204" # show pods as not ready if auto-unseal fails somehow or the node is not initialized yet
    livenessProbe:
      enabled: true # essential for vault to restart when the old certificate expires
    affinity: |
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app.kubernetes.io/name
                operator: In
                values:
                - {{ template "vault.name" . }}
              - key: app.kubernetes.io/instance
                operator: In
                values:
                - "{{ .Release.Name }}"
            topologyKey: kubernetes.io/hostname
    ha:
      enabled: true
      disruptionBudget:
        maxUnavailable: 1
      raft:
        enabled: true
        setNodeId: true # makes the raft node id be the same as the pod name for easier troubleshooting
        config: |
          ui = true
          disable_mlock = true  # necessary with integrated storage to play nice with the used BoltDB (may cause OOM if vault's dataset grows large), but causes additional vulnerability to memory attacks! -> disable swaps in OS!
          service_registration "kubernetes" {} # to get the 'active' kubernetes label on the active server pod for scheduling/routing
          listener "tcp" {
            address = "[::]:8200"
            cluster_address = "[::]:8201"
            tls_disable = true
            telemetry {
                unauthenticated_metrics_access = true
            }
          }
          
          storage "raft" {
            path = "/vault/data"
            retry_join {
              leader_api_addr = "http://vault-0.vault-internal:8200"
            }
            retry_join {
              leader_api_addr = "http://vault-1.vault-internal:8200"
            }
            retry_join {
              leader_api_addr = "http://vault-2.vault-internal:8200"
            }
            autopilot {
              cleanup_dead_servers = "true"
              last_contact_threshold = "5s"
              last_contact_failure_threshold = "10m"
              max_trailing_logs = 1024
              min_quorum = 2
              server_stabilization_time = "10s"
            }
          }
          seal "gcpckms" {
            credentials = "/vault/userconfig/unseal-creds/creds.json"
            project = "gepaplexx"
            region = "europe-west6"
            key_ring = "gepaplexx-keyring"
            crypto_key = "vault-key"
            disabled = "false"
          }
          telemetry {
            disable_hostname = true #'It is recommended to also enable the option disable_hostname to avoid having prefixed metrics with hostname.'
            prometheus_retention_time = "4h" 
          }
    resources:
      requests:
        memory: '100Mi'
      limits:
        memory: '150Mi'
    volumes:
      - name: unseal-creds
        secret:
          secretName: gcp-creds
          optional: false
    volumeMounts:
      - mountPath: /vault/userconfig/unseal-creds
        name: unseal-creds
        readOnly: true