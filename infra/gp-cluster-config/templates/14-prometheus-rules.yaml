##############################################################################
############################ OPENSHIFT API SERVER ############################
##############################################################################
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: alerts-openshift-apiserver
  namespace: openshift-apiserver
spec:
  groups:
    - name: alerts-openshift-apiserver
      rules:
        - alert: apiserver_podcount
          annotations:
            description: Es muss zumindest 1 Pod da sein
            summary: Apiserver {{`{{ printf "$labels.pod" }}`}} am {{ .Values.alerts.environment }}-Cluster nicht mehr erreichbar.
          expr: 'absent(sum(up{job="api"})>=1)'
          for: 10m
          labels:
            severity: "critical"
            namespace: "openshift-apiserver"
        - alert: apiserver_memory_saturation
          annotations:
            summary: Openshift Apiserver Memory Issue
            description: |
              Openshift Apiserver {{`{{ printf "$labels.pod" }}`}} hat in den letzten 10 Minuten durchschnittlich mehr als 250% der Memory Requests benötigt. Es ist sehr wahrscheinlich, dass hier ein Problem besteht.
          expr: |
            (sum(
            avg_over_time(container_memory_working_set_bytes{namespace="openshift-apiserver", container!="", image!=""}[10m])
            * on(namespace,pod)
            group_left(workload, workload_type) namespace_workload_pod:kube_pod_owner:relabel{ namespace="openshift-apiserver", workload="apiserver", workload_type="deployment"}
            ) by (pod)
            /sum(
            kube_pod_container_resource_requests{job="kube-state-metrics",namespace="openshift-apiserver", resource="memory"}
            * on(namespace,pod)
            group_left(workload, workload_type) namespace_workload_pod:kube_pod_owner:relabel{namespace="openshift-apiserver",workload="apiserver",workload_type="deployment"}
            ) by (pod)) > 2.5
          for: 10m
          labels:
            severity: "critical"
            namespace: "openshift-apiserver"
        - alert: apiserver_cpu_saturation
          annotations:
            summary: Openshift Apiserver CPU Issue
            description: |
              Openshift Apiserver {{`{{ printf "$labels.pod" }}`}} hat in den letzten 10 Minuten durchschnittlich mehr als 120% der requesten CPU verbraucht. Es ist sehr wahrscheinlich, dass hier ein Problem besteht.
          expr: |
            (sum(
                avg_over_time(node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate{namespace="openshift-apiserver"}[10m])
              * on(namespace,pod)
                group_left(workload, workload_type) namespace_workload_pod:kube_pod_owner:relabel{namespace="openshift-apiserver", workload="apiserver", workload_type="deployment"}
            ) by (pod)
            /sum(
                kube_pod_container_resource_requests{job="kube-state-metrics",namespace="openshift-apiserver", resource="cpu"}
              * on(namespace,pod)
                group_left(workload, workload_type) namespace_workload_pod:kube_pod_owner:relabel{namespace="openshift-apiserver", workload="apiserver", workload_type="deployment"}
            ) by (pod)) > 1.2
          for: 10m
          labels:
            severity: "critical"
            namespace: "openshift-apiserver"
---
##############################################################################
######################### OPENSHIFT KUBE API SERVER ##########################
##############################################################################
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: alerts-openshift-kube-apiserver
  namespace: openshift-kube-apiserver
spec:
  groups:
    - name: alerts-openshift-kube-apiserver
      rules:
        - alert: kubeapiserver_podcount
          annotations:
            description: Es muss zumindest 1 Pod da sein
            summary: Kube-Apiserver {{`{{ printf "$labels.pod" }}`}} am {{ .Values.alerts.environment }}-Cluster nicht mehr erreichbar.
          expr: 'absent(sum(up{job="apiserver"})>=1)'
          for: 10m
          labels:
            severity: "critical"
            namespace: "openshift-kube-apiserver"
        - alert: kubeapiserver_memory_saturation
          annotations:
            summary: Openshift Kube-Apiserver Memory Issue
            description: |
              Openshift Kube-Apiserver {{`{{ printf "$labels.pod" }}`}} hat in den letzten 10 Minuten durchschnittlich mehr als 800% der Memory Requests benötigt. Es ist sehr wahrscheinlich, dass hier ein Problem besteht.
          expr: |
            (sum(
            avg_over_time(container_memory_working_set_bytes{namespace="openshift-kube-apiserver", container="kube-apiserver", image!=""}[10m])
            ) by (pod)
            /sum(
            kube_pod_container_resource_requests{job="kube-state-metrics",namespace="openshift-kube-apiserver", resource="memory",container="kube-apiserver"}
            ) by (pod)) > 8
          for: 10m
          labels:
            severity: "critical"
            namespace: "openshift-kube-apiserver"
        - alert: kubeapiserver_cpu_saturation
          annotations:
            summary: Openshift Kube-Apiserver CPU Issue
            description: |
              Openshift Kube-Apiserver {{`{{ printf "$labels.pod" }}`}} hat in den letzten 10 Minuten durchschnittlich mehr als 500% der requesten CPU verbraucht. Es ist sehr wahrscheinlich, dass hier ein Problem besteht.
          expr: |
            (sum(
                avg_over_time(node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate{namespace="openshift-kube-apiserver",container="kube-apiserver"}[10m])
            ) by (pod)
            /sum(
                kube_pod_container_resource_requests{job="kube-state-metrics",namespace="openshift-kube-apiserver", resource="cpu", container="kube-apiserver"}
            ) by (pod)) > 5
          for: 10m
          labels:
            severity: "critical"
            namespace: "openshift-kube-apiserver"