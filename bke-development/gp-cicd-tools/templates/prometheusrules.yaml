{{- if and (.Values.argo_events.enabled) (.Values.argo_events.controller.metrics.serviceMonitor.enabled)  -}}
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: argo-events
  namespace: {{ .Release.Namespace }}
  labels:
    {{- include "gp-cicd-tools.labels" . | nindent 4 }}
spec:
  groups:
    - name: argo-events.rule
      rules:
        - alert: argo-events-controller-podcount
          annotations:
            summary: Argo Events Controller Manager ist nicht available.
            description: Es muss zumindest 1 Pod da sein
          expr: 'absent(sum(up{job=''argo-events-controller-manager-metrics''})>=1)'
          for: 10m
          labels:
            severity: "critical"
            type: "internal"
        - alert: argo-events-controller_memory_saturation
          annotations:
            summary: Argo Events Controller Manager Memory Issue
            description: |
              Argo Events Controller Manager hat in den letzten 10 Minuten durchschnittlich mehr als 150% der Memory Requests benötigt. Es ist sehr wahrscheinlich, dass hier ein Problem besteht.
          expr: |
            (sum(
                avg_over_time(container_memory_working_set_bytes{namespace="{{ .Release.Namespace }}", container!="", image!=""}[10m])
              * on(namespace,pod)
                group_left(workload, workload_type) namespace_workload_pod:kube_pod_owner:relabel{ namespace="{{ .Release.Namespace }}", workload="{{ template "argo-events.controller.fullname" .Subcharts.argo_events  }}", workload_type="deployment"}
            ) by (pod)
            /sum(
                kube_pod_container_resource_requests{job="kube-state-metrics",namespace="{{ .Release.Namespace }}", resource="memory"}
              * on(namespace,pod)
                group_left(workload, workload_type) namespace_workload_pod:kube_pod_owner:relabel{namespace="{{ .Release.Namespace }}",workload="{{ template "argo-events.controller.fullname" .Subcharts.argo_events  }}",workload_type="deployment"}
            ) by (pod)) > 1.5
          for: 10m
          labels:
            severity: "warning"
            type: "internal"
        - alert: argo-events-controller_memory_limit
          annotations:
            summary: Argo Events Controller Manager Memory Issue
            description: |
              Argo Events Controller Manager hat in den letzten 10 Minuten durchschnittlich mehr als 85% der Memory Limits benötigt. Es ist sehr wahrscheinlich, dass hier ein Problem besteht.
          expr: |
            (sum(
                avg_over_time(container_memory_working_set_bytes{namespace="{{ .Release.Namespace }}", container!="", image!=""}[10m])
              * on(namespace,pod)
                group_left(workload, workload_type) namespace_workload_pod:kube_pod_owner:relabel{ namespace="{{ .Release.Namespace }}", workload="{{ template "argo-events.controller.fullname" .Subcharts.argo_events  }}", workload_type="deployment"}
            ) by (pod)
            /sum(
                kube_pod_container_resource_limits{job="kube-state-metrics",namespace="{{ .Release.Namespace }}", resource="memory"}
              * on(namespace,pod)
                group_left(workload, workload_type) namespace_workload_pod:kube_pod_owner:relabel{namespace="{{ .Release.Namespace }}",workload="{{ template "argo-events.controller.fullname" .Subcharts.argo_events  }}",workload_type="deployment"}
            ) by (pod)) > 0.85
          for: 10m
          labels:
            severity: "warning"
            type: "internal"
        - alert: argo-events-controller_cpu_saturation
          annotations:
            summary: Argo Events Controller CPU Issue
            description: |
              argo-events-controller hat in den letzten 10 Minuten durchschnittlich mehr als 120% der requesten CPU verbraucht. Es ist sehr wahrscheinlich, dass hier ein Problem besteht.
          expr: |
            (sum(
                avg_over_time(node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate{namespace="{{ .Release.Namespace }}"}[10m])
              * on(namespace,pod)
                group_left(workload, workload_type) namespace_workload_pod:kube_pod_owner:relabel{namespace="{{ .Release.Namespace }}", workload="{{ template "argo-events.controller.fullname" .Subcharts.argo_events  }}", workload_type="deployment"}
            ) by (pod)
            /sum(
                kube_pod_container_resource_requests{job="kube-state-metrics",namespace="{{ .Release.Namespace }}", resource="cpu"}
              * on(namespace,pod)
                group_left(workload, workload_type) namespace_workload_pod:kube_pod_owner:relabel{namespace="{{ .Release.Namespace }}", workload="{{ template "argo-events.controller.fullname" .Subcharts.argo_events  }}", workload_type="deployment"}
            ) by (pod)) > 1.2
          for: 10m
          labels:
            severity: "warning"
            type: "internal"
        - alert: argo-events-controller_cpu_limit
          annotations:
            summary: Argo Events Controller CPU Issue
            description: |
              Argo Events Controller hat in den letzten 10 Minuten durchschnittlich mehr als 85% der limitierten CPU verbraucht. Es ist sehr wahrscheinlich, dass hier ein Problem besteht.
          expr: |
            (sum(
                avg_over_time(node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate{namespace="{{ .Release.Namespace }}"}[10m])
              * on(namespace,pod)
                group_left(workload, workload_type) namespace_workload_pod:kube_pod_owner:relabel{namespace="{{ .Release.Namespace }}", workload="{{ template "argo-events.controller.fullname" .Subcharts.argo_events  }}", workload_type="deployment"}
            ) by (pod)
            /sum(
                kube_pod_container_resource_limits{job="kube-state-metrics",namespace="{{ .Release.Namespace }}", resource="cpu"}
              * on(namespace,pod)
                group_left(workload, workload_type) namespace_workload_pod:kube_pod_owner:relabel{namespace="{{ .Release.Namespace }}", workload="{{ template "argo-events.controller.fullname" .Subcharts.argo_events  }}", workload_type="deployment"}
            ) by (pod)) > 0.85
          for: 10m
          labels:
            severity: "warning"
            type: "internal"
        - alert: argo-events-controller_workqueue_high
          annotations:
            summary: Argo Events Controller großer Backlog an Workqueue Items
            description: |
              Argo Events Controller hat eine Workqueue von 15 oder größer über mehr als 10 Minuten
          expr: |
            workqueue_depth{name="eventbus-controller"} > 10 OR
            workqueue_depth{name="eventsource-controller"} > 10  OR
            workqueue_depth{name="sensor-controller"} > 10
          for: 10m
          labels:
            severity: "warning"
            type: "internal"
---
{{- end }}
{{- if and (.Values.argo_workflows.enabled) (.Values.argo_workflows.controller.serviceMonitor.enabled) (.Values.argo_workflows.controller.metricsConfig.enabled)  }}
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: argo-workflows
  namespace: {{ .Release.Namespace }}
  labels:
    {{- include "gp-cicd-tools.labels" . | nindent 4 }}
spec:
  groups:
    - name: argo-workflows.rule
      rules:
        - alert: argo-workflows-podcount
          annotations:
            summary: Argo Workflows Controller ist nicht available.
            description: Es muss zumindest 1 Pod da sein
          expr: 'absent(sum(up{job=''argo-workflows-workflow-controller''})>=1)'
          for: 10m
          labels:
            severity: "critical"
            type: "internal"
        - alert: argo-workflows-controller_memory_saturation
          annotations:
            summary: Argo Workflows Controller Manager Memory Issue
            description: |
              Argo Workflows Controller Manager hat in den letzten 10 Minuten durchschnittlich mehr als 150% der Memory Requests benötigt. Es ist sehr wahrscheinlich, dass hier ein Problem besteht.
          expr: |
            (sum(
                avg_over_time(container_memory_working_set_bytes{namespace="{{ .Release.Namespace }}", container!="", image!=""}[10m])
              * on(namespace,pod)
                group_left(workload, workload_type) namespace_workload_pod:kube_pod_owner:relabel{ namespace="{{ .Release.Namespace }}", workload="{{ template "argo-workflows.controller.fullname" .Subcharts.argo_workflows  }}", workload_type="deployment"}
            ) by (pod)
            /sum(
                kube_pod_container_resource_requests{job="kube-state-metrics",namespace="{{ .Release.Namespace }}", resource="memory"}
              * on(namespace,pod)
                group_left(workload, workload_type) namespace_workload_pod:kube_pod_owner:relabel{namespace="{{ .Release.Namespace }}",workload="{{ template "argo-workflows.controller.fullname" .Subcharts.argo_workflows  }}",workload_type="deployment"}
            ) by (pod)) > 1.5
          for: 10m
          labels:
            severity: "warning"
            type: "internal"
        - alert: argo-workflows-controller_memory_limit
          annotations:
            summary: Argo Workflows Controller Manager Memory Issue
            description: |
              Argo Workflows Controller Manager hat in den letzten 10 Minuten durchschnittlich mehr als 85% der Memory Limits benötigt. Es ist sehr wahrscheinlich, dass hier ein Problem besteht.
          expr: |
            (sum(
                avg_over_time(container_memory_working_set_bytes{namespace="{{ .Release.Namespace }}", container!="", image!=""}[10m])
              * on(namespace,pod)
                group_left(workload, workload_type) namespace_workload_pod:kube_pod_owner:relabel{ namespace="{{ .Release.Namespace }}", workload="{{ template "argo-workflows.controller.fullname" .Subcharts.argo_workflows  }}", workload_type="deployment"}
            ) by (pod)
            /sum(
                kube_pod_container_resource_limits{job="kube-state-metrics",namespace="{{ .Release.Namespace }}", resource="memory"}
              * on(namespace,pod)
                group_left(workload, workload_type) namespace_workload_pod:kube_pod_owner:relabel{namespace="{{ .Release.Namespace }}",workload="{{ template "argo-workflows.controller.fullname" .Subcharts.argo_workflows  }}",workload_type="deployment"}
            ) by (pod)) > 0.85
          for: 10m
          labels:
            severity: "warning"
            type: "internal"
        - alert: argo-workflows-controller_cpu_saturation
          annotations:
            summary: Argo Workflows Controller CPU Issue
            description: |
              Argo Workflows Controller hat in den letzten 10 Minuten durchschnittlich mehr als 120% der requesten CPU verbraucht. Es ist sehr wahrscheinlich, dass hier ein Problem besteht.
          expr: |
            (sum(
                avg_over_time(node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate{namespace="{{ .Release.Namespace }}"}[10m])
              * on(namespace,pod)
                group_left(workload, workload_type) namespace_workload_pod:kube_pod_owner:relabel{namespace="{{ .Release.Namespace }}", workload="{{ template "argo-workflows.controller.fullname" .Subcharts.argo_workflows  }}", workload_type="deployment"}
            ) by (pod)
            /sum(
                kube_pod_container_resource_requests{job="kube-state-metrics",namespace="{{ .Release.Namespace }}", resource="cpu"}
              * on(namespace,pod)
                group_left(workload, workload_type) namespace_workload_pod:kube_pod_owner:relabel{namespace="{{ .Release.Namespace }}", workload="{{ template "argo-workflows.controller.fullname" .Subcharts.argo_workflows  }}", workload_type="deployment"}
            ) by (pod)) > 1.2
          for: 10m
          labels:
            severity: "warning"
            type: "internal"
        - alert: argo-workflows-controller_cpu_limit
          annotations:
            summary: Argo Workflows Controller CPU Issue
            description: |
              Argo Workflows Controller hat in den letzten 10 Minuten durchschnittlich mehr als 85% der limitierten CPU verbraucht. Es ist sehr wahrscheinlich, dass hier ein Problem besteht.
          expr: |
            (sum(
                avg_over_time(node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate{namespace="{{ .Release.Namespace }}"}[10m])
              * on(namespace,pod)
                group_left(workload, workload_type) namespace_workload_pod:kube_pod_owner:relabel{namespace="{{ .Release.Namespace }}", workload="{{ template "argo-workflows.controller.fullname" .Subcharts.argo_workflows  }}", workload_type="deployment"}
            ) by (pod)
            /sum(
                kube_pod_container_resource_limits{job="kube-state-metrics",namespace="{{ .Release.Namespace }}", resource="cpu"}
              * on(namespace,pod)
                group_left(workload, workload_type) namespace_workload_pod:kube_pod_owner:relabel{namespace="{{ .Release.Namespace }}", workload="{{ template "argo-workflows.controller.fullname" .Subcharts.argo_workflows  }}", workload_type="deployment"}
            ) by (pod)) > 0.85
          for: 10m
          labels:
            severity: "warning"
            type: "internal"
        - alert: workflow_failed
          annotations:
            description: |-
              Applikation: {{ $labels.application }}
              Branch: {{ $labels.branch }}
              Commiter: {{ $labels.commit_user }} <{{ $labels.commit_email }}>
            summary: 'Workflow "{{ $labels.task_name }}" Status: {{ $labels.status }}'
          expr: >-
            rate(argo_workflows_result_task_counter{status!="Succeeded"}[1m]) !=
            0
          labels:
            type: argo-workflows-fail
            severity: critical
---
{{- end }}
{{- if .Values.argocd.enabled }} # ArgoCD wird über den OpenShift-Gitops Operator deployed. Dieser deployed standardmößig 3 Servicemonitor für ArgoCD Server, Repo Server und ArgoCD selbst mit.
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: argo-cd
  namespace: {{ .Release.Namespace }}
  labels:
    {{- include "gp-cicd-tools.labels" . | nindent 4 }}
spec:
  groups:
    - name: argo-cd.rule
      rules:
        - alert: argo-cd-podcount
          annotations:
            summary: Argo CD Application-Controller ist nicht available.
            description: Es muss zumindest 1 Pod da sein
          expr: 'absent(sum(up{job=''{{ template "gp-cicd-tools.argocd.servicemonitor" . }}''})>=1)'
          for: 10m
          labels:
            severity: "critical"
            type: "internal"
        - alert: argocd_memory_saturation
          annotations:
            summary: Argo CD Application-Controller Memory Issue
            description: |
              Argo CD hat in den letzten 10 Minuten durchschnittlich mehr als 150% der Memory Requests benötigt. Es ist sehr wahrscheinlich, dass hier ein Problem besteht.
          expr: |
            (sum(
                avg_over_time(container_memory_working_set_bytes{namespace="{{ .Release.Namespace }}", container!="", image!=""}[10m])
              * on(namespace,pod)
                group_left(workload, workload_type) namespace_workload_pod:kube_pod_owner:relabel{ namespace="{{ .Release.Namespace }}", workload="{{ template "gp-cicd-tools.argocd.workload" . }}", workload_type="deployment"}
            ) by (pod)
            /sum(
                kube_pod_container_resource_requests{job="kube-state-metrics",namespace="{{ .Release.Namespace }}", resource="memory"}
              * on(namespace,pod)
                group_left(workload, workload_type) namespace_workload_pod:kube_pod_owner:relabel{namespace="{{ .Release.Namespace }}",workload="{{ template "gp-cicd-tools.argocd.workload" . }}",workload_type="deployment"}
            ) by (pod)) > 1.5
          for: 10m
          labels:
            severity: "warning"
            type: "internal"
        - alert: argocd_memory_limit
          annotations:
            summary: Argo CD Application-Controller Memory Issue
            description: |
              Argo CD Application-Controller hat in den letzten 10 Minuten durchschnittlich mehr als 85% der Memory Limits benötigt. Es ist sehr wahrscheinlich, dass hier ein Problem besteht.
          expr: |
            (sum(
                avg_over_time(container_memory_working_set_bytes{namespace="{{ .Release.Namespace }}", container!="", image!=""}[10m])
              * on(namespace,pod)
                group_left(workload, workload_type) namespace_workload_pod:kube_pod_owner:relabel{ namespace="{{ .Release.Namespace }}", workload="{{ template "gp-cicd-tools.argocd.workload" . }}", workload_type="deployment"}
            ) by (pod)
            /sum(
                kube_pod_container_resource_limits{job="kube-state-metrics",namespace="{{ .Release.Namespace }}", resource="memory"}
              * on(namespace,pod)
                group_left(workload, workload_type) namespace_workload_pod:kube_pod_owner:relabel{namespace="{{ .Release.Namespace }}",workload="{{ template "gp-cicd-tools.argocd.workload" . }}",workload_type="deployment"}
            ) by (pod)) > 0.85
          for: 10m
          labels:
            severity: "warning"
            type: "internal"
        - alert: argocd_cpu_saturation
          annotations:
            summary: Argo CD Application-Controller CPU Issue
            description: |
              Argo CD Application-Controller hat in den letzten 10 Minuten durchschnittlich mehr als 120% der requesten CPU verbraucht. Es ist sehr wahrscheinlich, dass hier ein Problem besteht.
          expr: |
            (sum(
                avg_over_time(node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate{namespace="{{ .Release.Namespace }}"}[10m])
              * on(namespace,pod)
                group_left(workload, workload_type) namespace_workload_pod:kube_pod_owner:relabel{namespace="{{ .Release.Namespace }}", workload="{{ template "gp-cicd-tools.argocd.workload" . }}", workload_type="deployment"}
            ) by (pod)
            /sum(
                kube_pod_container_resource_requests{job="kube-state-metrics",namespace="{{ .Release.Namespace }}", resource="cpu"}
              * on(namespace,pod)
                group_left(workload, workload_type) namespace_workload_pod:kube_pod_owner:relabel{namespace="{{ .Release.Namespace }}", workload="{{ template "gp-cicd-tools.argocd.workload" . }}", workload_type="deployment"}
            ) by (pod)) > 1.2
          for: 10m
          labels:
            severity: "warning"
            type: "internal"
        - alert: argocd_cpu_limit
          annotations:
            summary: Argo CD Application-Controller CPU Issue
            description: |
              Argo CD Application-Controller hat in den letzten 10 Minuten durchschnittlich mehr als 85% der limitierten CPU verbraucht. Es ist sehr wahrscheinlich, dass hier ein Problem besteht.
          expr: |
            (sum(
                avg_over_time(node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate{namespace="{{ .Release.Namespace }}"}[10m])
              * on(namespace,pod)
                group_left(workload, workload_type) namespace_workload_pod:kube_pod_owner:relabel{namespace="{{ .Release.Namespace }}", workload="{{ template "gp-cicd-tools.argocd.workload" . }}", workload_type="deployment"}
            ) by (pod)
            /sum(
                kube_pod_container_resource_limits{job="kube-state-metrics",namespace="{{ .Release.Namespace }}", resource="cpu"}
              * on(namespace,pod)
                group_left(workload, workload_type) namespace_workload_pod:kube_pod_owner:relabel{namespace="{{ .Release.Namespace }}", workload="{{ template "gp-cicd-tools.argocd.workload" . }}", workload_type="deployment"}
            ) by (pod)) > 0.85
          for: 10m
          labels:
            severity: "warning"
            type: "internal"
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: argo-cd-reposerver
  namespace: {{ .Release.Namespace }}
  labels:
    {{- include "gp-cicd-tools.labels" . | nindent 4 }}
spec:
  groups:
    - name: argo-cd-reposerver.rule
      rules:
        - alert: argo-cd-reposerver-podcount
          annotations:
            summary: ArgoCD Reposerver ist nicht available.
            description: Es muss zumindest 1 Pod da sein
          expr: 'absent(sum(up{job=''{{ template "gp-cicd-tools.argocd-repo-server.servicemonitor" . }}''})>=1)'
          for: 10m
          labels:
            severity: "critical"
            type: "internal"
        - alert: argocd-reposerver_memory_saturation
          annotations:
            summary: Argo CD Reposerver Memory Issue
            description: |
              Argo CD Reposerver hat in den letzten 10 Minuten durchschnittlich mehr als 150% der Memory Requests benötigt. Es ist sehr wahrscheinlich, dass hier ein Problem besteht.
          expr: |
            (sum(
                avg_over_time(container_memory_working_set_bytes{namespace="{{ .Release.Namespace }}", container!="", image!=""}[10m])
              * on(namespace,pod)
                group_left(workload, workload_type) namespace_workload_pod:kube_pod_owner:relabel{ namespace="{{ .Release.Namespace }}", workload="{{ template "gp-cicd-tools.argocd-repo-server.workload" . }}", workload_type="deployment"}
            ) by (pod)
            /sum(
                kube_pod_container_resource_requests{job="kube-state-metrics",namespace="{{ .Release.Namespace }}", resource="memory"}
              * on(namespace,pod)
                group_left(workload, workload_type) namespace_workload_pod:kube_pod_owner:relabel{namespace="{{ .Release.Namespace }}",workload="{{ template "gp-cicd-tools.argocd-repo-server.workload" . }}",workload_type="deployment"}
            ) by (pod)) > 1.5
          for: 10m
          labels:
            severity: "warning"
            type: "internal"
        - alert: argocd-reposerver_memory_limit
          annotations:
            summary: Argo CD Reposerver Memory Issue
            description: |
              Argo CD Reposerver hat in den letzten 10 Minuten durchschnittlich mehr als 85% der Memory Limits benötigt. Es ist sehr wahrscheinlich, dass hier ein Problem besteht.
          expr: |
            (sum(
                avg_over_time(container_memory_working_set_bytes{namespace="{{ .Release.Namespace }}", container!="", image!=""}[10m])
              * on(namespace,pod)
                group_left(workload, workload_type) namespace_workload_pod:kube_pod_owner:relabel{ namespace="{{ .Release.Namespace }}", workload="{{ template "gp-cicd-tools.argocd-repo-server.workload" . }}", workload_type="deployment"}
            ) by (pod)
            /sum(
                kube_pod_container_resource_limits{job="kube-state-metrics",namespace="{{ .Release.Namespace }}", resource="memory"}
              * on(namespace,pod)
                group_left(workload, workload_type) namespace_workload_pod:kube_pod_owner:relabel{namespace="{{ .Release.Namespace }}",workload="{{ template "gp-cicd-tools.argocd-repo-server.workload" . }}",workload_type="deployment"}
            ) by (pod)) > 0.85
          for: 10m
          labels:
            severity: "warning"
            type: "internal"
        - alert: argocd-reposerver_cpu_saturation
          annotations:
            summary: Argo CD Reposerver CPU Issue
            description: |
              Argo CD Reposerver hat in den letzten 10 Minuten durchschnittlich mehr als 120% der requesten CPU verbraucht. Es ist sehr wahrscheinlich, dass hier ein Problem besteht.
          expr: |
            (sum(
                avg_over_time(node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate{namespace="{{ .Release.Namespace }}"}[10m])
              * on(namespace,pod)
                group_left(workload, workload_type) namespace_workload_pod:kube_pod_owner:relabel{namespace="{{ .Release.Namespace }}", workload="{{ template "gp-cicd-tools.argocd-repo-server.workload" . }}", workload_type="deployment"}
            ) by (pod)
            /sum(
                kube_pod_container_resource_requests{job="kube-state-metrics",namespace="{{ .Release.Namespace }}", resource="cpu"}
              * on(namespace,pod)
                group_left(workload, workload_type) namespace_workload_pod:kube_pod_owner:relabel{namespace="{{ .Release.Namespace }}", workload="{{ template "gp-cicd-tools.argocd-repo-server.workload" . }}", workload_type="deployment"}
            ) by (pod)) > 1.2
          for: 10m
          labels:
            severity: "warning"
            type: "internal"
        - alert: argocd-reposerver_cpu_limit
          annotations:
            summary: Argo CD CPU Issue
            description: |
              Argo CD Reposerver hat in den letzten 10 Minuten durchschnittlich mehr als 85% der limitierten CPU verbraucht. Es ist sehr wahrscheinlich, dass hier ein Problem besteht.
          expr: |
            (sum(
                avg_over_time(node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate{namespace="{{ .Release.Namespace }}"}[10m])
              * on(namespace,pod)
                group_left(workload, workload_type) namespace_workload_pod:kube_pod_owner:relabel{namespace="{{ .Release.Namespace }}", workload="{{ template "gp-cicd-tools.argocd-repo-server.workload" . }}", workload_type="deployment"}
            ) by (pod)
            /sum(
                kube_pod_container_resource_limits{job="kube-state-metrics",namespace="{{ .Release.Namespace }}", resource="cpu"}
              * on(namespace,pod)
                group_left(workload, workload_type) namespace_workload_pod:kube_pod_owner:relabel{namespace="{{ .Release.Namespace }}", workload="{{ template "gp-cicd-tools.argocd-repo-server.workload" . }}", workload_type="deployment"}
            ) by (pod)) > 0.85
          for: 10m
          labels:
            severity: "warning"
            type: "internal"
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: argo-cd-server
  namespace: {{ .Release.Namespace }}
  labels:
    {{- include "gp-cicd-tools.labels" . | nindent 4 }}
spec:
  groups:
    - name: argo-cd-server.rule
      rules:
        - alert: argo-cd-server-podcount
          annotations:
            summary: ArgoCD Server ist nicht available.
            description: Es muss zumindest 1 Pod da sein
          expr: 'absent(sum(up{job=''{{ template "gp-cicd-tools.argocd-server.servicemonitor" . }}''})>=1)'
          for: 10m
          labels:
            severity: "critical"
            type: "internal"
        - alert: argocd-server_memory_saturation
          annotations:
            summary: Argo CD Server Memory Issue
            description: |
              Argo CD Server hat in den letzten 10 Minuten durchschnittlich mehr als 150% der Memory Requests benötigt. Es ist sehr wahrscheinlich, dass hier ein Problem besteht.
          expr: |
            (sum(
                avg_over_time(container_memory_working_set_bytes{namespace="{{ .Release.Namespace }}", container!="", image!=""}[10m])
              * on(namespace,pod)
                group_left(workload, workload_type) namespace_workload_pod:kube_pod_owner:relabel{ namespace="{{ .Release.Namespace }}", workload="{{ template "gp-cicd-tools.argocd-server.workload" . }}", workload_type="deployment"}
            ) by (pod)
            /sum(
                kube_pod_container_resource_requests{job="kube-state-metrics",namespace="{{ .Release.Namespace }}", resource="memory"}
              * on(namespace,pod)
                group_left(workload, workload_type) namespace_workload_pod:kube_pod_owner:relabel{namespace="{{ .Release.Namespace }}",workload="{{ template "gp-cicd-tools.argocd-server.workload" . }}",workload_type="deployment"}
            ) by (pod)) > 1.5
          for: 10m
          labels:
            severity: "warning"
            type: "internal"
        - alert: argocd-server_memory_limit
          annotations:
            summary: Argo CD Server Memory Issue
            description: |
              Argo CD Server hat in den letzten 10 Minuten durchschnittlich mehr als 85% der Memory Limits benötigt. Es ist sehr wahrscheinlich, dass hier ein Problem besteht.
          expr: |
            (sum(
                avg_over_time(container_memory_working_set_bytes{namespace="{{ .Release.Namespace }}", container!="", image!=""}[10m])
              * on(namespace,pod)
                group_left(workload, workload_type) namespace_workload_pod:kube_pod_owner:relabel{ namespace="{{ .Release.Namespace }}", workload="{{ template "gp-cicd-tools.argocd-server.workload" . }}", workload_type="deployment"}
            ) by (pod)
            /sum(
                kube_pod_container_resource_limits{job="kube-state-metrics",namespace="{{ .Release.Namespace }}", resource="memory"}
              * on(namespace,pod)
                group_left(workload, workload_type) namespace_workload_pod:kube_pod_owner:relabel{namespace="{{ .Release.Namespace }}",workload="{{ template "gp-cicd-tools.argocd-server.workload" . }}",workload_type="deployment"}
            ) by (pod)) > 0.85
          for: 10m
          labels:
            severity: "warning"
            type: "internal"
        - alert: argocd-reposerver_cpu_saturation
          annotations:
            summary: Argo CD Server CPU Issue
            description: |
              Argo CD Server hat in den letzten 10 Minuten durchschnittlich mehr als 120% der requesten CPU verbraucht. Es ist sehr wahrscheinlich, dass hier ein Problem besteht.
          expr: |
            (sum(
                avg_over_time(node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate{namespace="{{ .Release.Namespace }}"}[10m])
              * on(namespace,pod)
                group_left(workload, workload_type) namespace_workload_pod:kube_pod_owner:relabel{namespace="{{ .Release.Namespace }}", workload="{{ template "gp-cicd-tools.argocd-server.workload" . }}", workload_type="deployment"}
            ) by (pod)
            /sum(
                kube_pod_container_resource_requests{job="kube-state-metrics",namespace="{{ .Release.Namespace }}", resource="cpu"}
              * on(namespace,pod)
                group_left(workload, workload_type) namespace_workload_pod:kube_pod_owner:relabel{namespace="{{ .Release.Namespace }}", workload="{{ template "gp-cicd-tools.argocd-server.workload" . }}", workload_type="deployment"}
            ) by (pod)) > 1.2
          for: 10m
          labels:
            severity: "warning"
            type: "internal"
        - alert: argocd-server_cpu_limit
          annotations:
            summary: Argo CD Server CPU Issue
            description: |
              Argo CD Server hat in den letzten 10 Minuten durchschnittlich mehr als 85% der limitierten CPU verbraucht. Es ist sehr wahrscheinlich, dass hier ein Problem besteht.
          expr: |
            (sum(
                avg_over_time(node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate{namespace="{{ .Release.Namespace }}"}[10m])
              * on(namespace,pod)
                group_left(workload, workload_type) namespace_workload_pod:kube_pod_owner:relabel{namespace="{{ .Release.Namespace }}", workload="{{ template "gp-cicd-tools.argocd-server.workload" . }}", workload_type="deployment"}
            ) by (pod)
            /sum(
                kube_pod_container_resource_limits{job="kube-state-metrics",namespace="{{ .Release.Namespace }}", resource="cpu"}
              * on(namespace,pod)
                group_left(workload, workload_type) namespace_workload_pod:kube_pod_owner:relabel{namespace="{{ .Release.Namespace }}", workload="{{ template "gp-cicd-tools.argocd-server.workload" . }}", workload_type="deployment"}
            ) by (pod)) > 0.85
          for: 10m
          labels:
            severity: "warning"
            type: "internal"
---
{{- end }}

